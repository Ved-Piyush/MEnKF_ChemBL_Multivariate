{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 11:56:24.851199: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-07 11:56:24.854709: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-07 11:56:24.892139: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-07 11:56:24.894117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 11:56:25.767910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df669ac-cde2-4008-b431-0903f21dce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, batch_data1, initial_ensembles, size_ens): \n",
    "\n",
    "    weights_ann_1 = ann_15.get_weights()\n",
    "    weights_ann_2 = ann_20.get_weights()\n",
    "    \n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "\n",
    "\n",
    "\n",
    "    n_hidden_2 = len(weights_ann_2[0].ravel())\n",
    "\n",
    "    initial_ensembles_1 = initial_ensembles.copy()[:, total_weights_1:(total_weights_1+ total_weights_2)]\n",
    "\n",
    "    hidden_weights_2 = initial_ensembles_1[:,:n_hidden_2].reshape(size_ens, batch_data1.shape[1], h2)\n",
    "\n",
    "\n",
    "\n",
    "    hidden_output_2 = np.einsum('ij,kjl->kil', batch_data1, hidden_weights_2)\n",
    "\n",
    "    hidden_layer_bias_2 = initial_ensembles_1[:,n_hidden_2:(n_hidden_2 + h2)].reshape(size_ens, 1,  h2)\n",
    "\n",
    "\n",
    "    # hidden_layer_bias_2 = np.expand_dims(hidden_layer_bias_2, 0)\n",
    "\n",
    "    hidden_output_2 = hidden_output_2+ hidden_layer_bias_2\n",
    "#     hidden_layer_bias_2 = hidden_layer_bias_2 + hidden_layer_bias_2\n",
    "\n",
    "\n",
    "    n_pred_weights_2 = len(weights_ann_2[2].ravel())\n",
    "\n",
    "    output_weights_2 = initial_ensembles_1[:,(n_hidden_2 + h2):(n_hidden_2 + h2 + n_pred_weights_2) ].reshape(size_ens, h2, target_dim)\n",
    "\n",
    "\n",
    "    output_2 = np.einsum('ijk,ikl->ijl', hidden_output_2, output_weights_2)\n",
    "\n",
    "\n",
    "    output_layer_bias_2 = initial_ensembles_1[:,(n_hidden_2 + h2 + n_pred_weights_2):(n_hidden_2 + h2 + n_pred_weights_2 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "    # output_layer_bias_2 = np.expand_dims(output_layer_bias_2, 0)\n",
    "\n",
    "\n",
    "    final_output_2 = output_2 + output_layer_bias_2\n",
    "\n",
    "\n",
    "    weights_1 = initial_ensembles[:, :total_weights_1]\n",
    "\n",
    "    weights_2 = initial_ensembles[:, total_weights_1:(total_weights_1 + total_weights_2)]\n",
    "\n",
    "#     print(weights_1.shape)\n",
    "    \n",
    "    # print(weights_2.shape)\n",
    "\n",
    "    avg_weights = initial_ensembles[:, -4].reshape(-1,1)\n",
    "    \n",
    "    cov_matrix_parms =  initial_ensembles[:, -3:].reshape(-1,1)\n",
    "    \n",
    "    # avg_weights_not_sig = avg_weights.reshape(avg_weights.shape[0], 1, avg_weights.shape[1])\n",
    "    \n",
    "    \n",
    "    avg_weights_sig = expit(avg_weights)\n",
    "    \n",
    "    avg_weights_sig = avg_weights_sig.reshape(avg_weights_sig.shape[0], 1, avg_weights_sig.shape[1])\n",
    "    \n",
    "    \n",
    "    complement_weights_sig = 1 - expit(avg_weights)\n",
    "    \n",
    "    complement_weights_sig = complement_weights_sig.reshape(complement_weights_sig.shape[0], 1, complement_weights_sig.shape[1])\n",
    "    \n",
    "\n",
    "    final_output_1 = final_output_1*complement_weights_sig\n",
    "    \n",
    "    final_output_2 = final_output_2*avg_weights_sig\n",
    "    \n",
    "    return final_output_1 + final_output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 2): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809d88a0-ecd2-44b9-9b94-2ece04ebc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f382d3e-e4dc-4e19-b19a-0eb954236a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "alogp_bottleneck = np.load(\"..//Data/small_mol_phase_3_features_for_both.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ccbcd4-20bd-4772-ac63-3aaef7ab95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e5e3f5-e4c6-40f8-ac40-b487543e3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = pd.read_csv(\"..//Data/smiles_with_rdkit_with_small_phase_3_outputs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de38304b-6569-4387-8868-0837b195e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b697b7f9-3d50-4d71-a436-cdc6434909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "std_targets = pickle.load( open('..//Data//target_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40aa1581-02a9-4c7f-a348-db236a77f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad7f00f-1bd3-40bc-b50b-de0406c72655",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_valid.values[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3fe10a7-fcc3-430f-8ade-356a7f7b85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77fe79e-61fc-40f4-a7d9-62de18572b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_t = np.cov(y_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfffa08-081b-4b5f-8181-ae94a920c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03896e4-b7c8-4237-a0a8-7af9ed6b1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(alogp_bottleneck, y_train, test_size = 0.25, shuffle = True, \n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ed02ca-e8a1-4767-8152-4fc17b1afe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(719, 64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88095659-0253-45ce-bdd9-ba45331b0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa0c9cb8-85f5-4bcb-96a2-cb8f9b917689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = x_train.shape[0]\n",
    "# R_t = block_diag(*([R_t] * n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e064cec-711a-451b-b99d-d9ae5fb164e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b98c57b-e85d-46f9-92b4-09f2a1b179a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_train = x_train[:, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b91f4be-60b4-44ee-b992-80da2cbe5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_feats_train = x_train[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe0b04f-b7c9-4f72-ac4b-4ba7e50d128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_valid = x_valid[:, :32]\n",
    "rdkit_feats_valid = x_valid[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c77cd415-1b34-404b-8280-b1de094e2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, h2 = 16, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72ab4c85-9ab7-485e-a96f-7fad3109beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 11:56:27.005584: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-07 11:56:27.005610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: c2127.swan.hcc.unl.edu\n",
      "2023-06-07 11:56:27.005615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: c2127.swan.hcc.unl.edu\n",
      "2023-06-07 11:56:27.005680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.85.5\n",
      "2023-06-07 11:56:27.005693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.85.5\n",
      "2023-06-07 11:56:27.005696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.85.5\n"
     ]
    }
   ],
   "source": [
    "ann_15 = ann(h1, 32, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e39d6d9b-f407-4b9a-89c3-c7443d076c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_20 = ann(h2, 32, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9f13bf6-b578-4c5e-a919-a7793ed67af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights_1 = ann_15.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "578be109-9a6c-4644-a20f-9842c75803e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights_2 = ann_20.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b76859a-1021-4391-bb40-8c052dcb5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = total_weights_1 + total_weights_2 + 1 + 3 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2025cd79-c8c6-4ce5-bcc6-59d72b2b8711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b074ab35-2e83-44f1-a96f-3679b7da5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate MVN from 0 and some var\n",
    "initial_ensembles_mean = np.zeros((total_weights,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83507ac0-1118-4fa6-8dc9-22c0aa978589",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f4120c3-5c1c-4471-b28d-3fefc0e55516",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ensembles_cov = lambda1*np.identity(total_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bea39262-6961-4750-841e-d25a8a2f3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b3b2f4d-263d-434f-b813-f7d7f3958816",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ensembles = mvn(initial_ensembles_mean, initial_ensembles_cov).rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0f2e620-c01f-43fa-b476-9b7001581f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 1130)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ensembles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8298f02a-cd1e-4594-82b0-9c6aa218a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_ensembles[0, -5:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9674c3af-3299-4556-884b-18c26dddf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles, idx):\n",
    "    cov_part = initial_ensembles[idx, -5:-2]\n",
    "    # variances = tf.math.softplus(cov_part[:2]).numpy()\n",
    "    variances = cov_part[:2]\n",
    "    covariances = cov_part[-1]\n",
    "    base_cov = np.identity(2)\n",
    "    base_cov[0,0] = variances[0]\n",
    "    base_cov[1,1] = variances[1]\n",
    "    base_cov[0,1] = covariances\n",
    "    base_cov[1,0] = covariances\n",
    "    \n",
    "    variances = tf.math.softplus(initial_ensembles[idx, -2:]).numpy()\n",
    "    base_variances = np.identity(2)\n",
    "    base_variances[0,0] = variances[0]\n",
    "    base_variances[1,1] = variances[1]\n",
    "    \n",
    "    final = np.linalg.cholesky(base_cov@base_cov.T + base_variances)\n",
    "    cov_mat = final@final.T\n",
    "    cov_mat_final = cov_mat@cov_mat\n",
    "    \n",
    "    n = shape\n",
    "    R_t = block_diag(*([cov_mat_final] * n))\n",
    "    \n",
    "    lambda_inv = np.linalg.inv(R_t)\n",
    "    \n",
    "    return lambda_inv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9956a3bb-dee9-4616-a93c-390acafda339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.det(np.array(all_covs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22464d8a-8440-45fc-a02e-17b1d58a4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.softplus(-0.2043372)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "325c2c9f-61c3-41eb-8518-72772303d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_ensembles[:, -3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cfec67f-202d-4aaf-ab01-58298b60140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_covs = np.array(all_covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "301947d7-fe9f-4635-ab07-a2fa2e7fe949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_covs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, initial_ensembles):\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    G_u = get_targets_with_weights(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    G_u = G_u.reshape(G_u.shape[0], G_u.shape[1]*G_u.shape[2])\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, initial_ensembles):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    all_covs = Parallel(n_jobs = 15, verbose = 0)(delayed(create_cov)(data1.shape[0],initial_ensembles, idx) for idx in range(size_ens))\n",
    "    all_covs = np.array(all_covs)\n",
    "    D_plus_cov = (D + all_covs)\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    # mid_quant = C@np.linalg.inv(D + lambda_inv)\n",
    "    right_quant = y_train.ravel().reshape(-1,1) - G_u.T\n",
    "    mid_times_right = mid_quant@right_quant\n",
    "    updated_ensemble = (initial_ensembles.T + mid_times_right)\n",
    "    updated_ensemble = updated_ensemble.mean(0)\n",
    "    return updated_ensemble.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "346d07bc-5b61-4e75-a816-a6e22ffe9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18b0d48e-f557-4d11-acb8-8905f403e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_bar, G_bar, G_u = calculate_mu_bar_G_bar(smiles_feats_train, rdkit_feats_train, initial_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9183ef8c-1730-4074-a982-680af10c67f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1438, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc3a216a-4dbb-474d-8219-5f6c071250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data, idx):\n",
    "    data_cur = data[idx, :, :]\n",
    "    inv_data_cur = std_targets.inverse_transform(data_cur)\n",
    "    return inv_data_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f151faf-ada0-4d56-87b8-6c96aa4eb0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch = Parallel(n_jobs = 15, verbose = 3)(delayed(inverse_transform)(G_u_test, i)  for i in range(G_u_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.12247029 116.83346494] [1. 1.] [ 100.2427185  2285.94471715]\n",
      "[  2.00366785 103.11753901] [1. 1.] [  98.4813168  2270.17568915]\n",
      "[ 2.06990245 74.54409833] [1. 1.] [  68.57682765 1562.86566629]\n",
      "[ 2.10250172 69.49683215] [1. 1.] [  68.20513281 1548.27782088]\n",
      "[ 1.83806057 58.94566369] [1. 1.] [  55.27521812 1255.38322481]\n",
      "[ 1.82686523 54.91518436] [1. 1.] [  54.71043213 1249.53001387]\n",
      "[ 1.87412405 66.84643257] [1. 1.] [  48.27196587 1121.66816016]\n",
      "[ 1.8302688  61.81765731] [1. 1.] [  48.049355   1116.74301632]\n",
      "[ 2.15255896 61.15247827] [1. 1.] [ 42.85634852 950.18270847]\n",
      "[ 2.12896543 56.86872526] [1. 1.] [ 42.57586462 937.55193879]\n",
      "[ 2.13739013 58.74039793] [1. 1.] [ 39.30210972 874.10011402]\n",
      "[ 2.1234225  54.60104055] [1. 1.] [ 38.70131786 866.52885702]\n",
      "[ 2.13685021 56.35677419] [1. 1.] [ 35.04597311 806.87831259]\n",
      "[ 2.12027654 52.41927041] [1. 1.] [ 34.53077507 795.31045446]\n",
      "[ 2.04235616 53.66420587] [1. 1.] [ 32.26104244 730.03689182]\n",
      "[ 2.04859937 49.76434662] [1. 1.] [ 31.9029419  720.89275971]\n",
      "[ 2.13698528 51.37474477] [1. 1.] [ 28.82485728 662.2780713 ]\n",
      "[ 2.11831298 47.85642954] [1. 1.] [ 28.63308499 651.11119987]\n",
      "[ 2.16027872 50.00661237] [1. 1.] [ 26.01491716 577.35884469]\n",
      "[ 2.12970716 46.5472366 ] [1. 1.] [ 25.67484059 568.55628526]\n",
      "[ 2.12401181 48.39553208] [1. 1.] [ 23.49794969 515.81435202]\n",
      "[ 2.09953185 45.25073567] [1. 1.] [ 23.18330468 510.07914912]\n",
      "[ 2.0517629  46.64195708] [1. 1.] [ 21.40392046 449.89632398]\n",
      "[ 2.00971914 43.81297506] [1. 1.] [ 21.06091333 444.81370306]\n",
      "[ 2.12360399 42.87409517] [1. 1.] [ 18.73318398 404.20629059]\n",
      "[ 2.05462034 40.4592464 ] [1. 1.] [ 18.41617568 397.57632985]\n",
      "[ 2.18896352 35.33451617] [1. 1.] [ 15.41364872 342.75662256]\n",
      "[ 2.10516625 33.46829825] [1. 1.] [ 15.19816361 336.4633276 ]\n",
      "[ 2.10888151 31.91918673] [1. 1.] [ 13.08507957 286.79249605]\n",
      "[ 2.02779551 30.56577484] [1. 1.] [ 12.96250755 282.7528698 ]\n",
      "[ 1.99211421 22.86334737] [1. 1.] [ 10.67866336 238.30509579]\n",
      "[ 1.94812151 21.92311153] [1. 1.] [ 10.61043589 236.23242791]\n",
      "[ 1.85722537 17.73084134] [1. 1.] [  9.48990709 202.21652292]\n",
      "[ 1.82158062 17.13624095] [1. 1.] [  9.32416802 199.44542542]\n",
      "[ 1.75405257 12.02918404] [1. 1.] [  7.34352416 158.11283722]\n",
      "[ 1.72191134 11.72160269] [1. 1.] [  7.18986025 156.37068824]\n",
      "[ 1.17759325 11.16172455] [1. 1.] [  5.73823551 126.60767838]\n",
      "[ 1.17841217 10.81856769] [1. 1.] [  5.67747721 125.07624347]\n",
      "[0.90739218 8.67590036] [1. 1.] [  4.60235009 101.45354944]\n",
      "[0.89558804 8.41884515] [1. 1.] [  4.53384386 100.47757949]\n",
      "[0.55678206 5.88394359] [1. 1.] [ 3.44984825 71.2759614 ]\n",
      "[0.5662637  5.59824519] [1. 1.] [ 3.40188171 70.71906738]\n",
      "[0.43265156 3.89155276] [1. 1.] [ 2.67552211 54.34197204]\n",
      "[0.42994537 3.7805407 ] [1. 1.] [ 2.65155282 54.03264976]\n",
      "[0.25913478 3.23054725] [1. 1.] [ 2.05648308 41.37020713]\n",
      "[0.24723648 3.20539623] [1. 1.] [ 2.03257947 41.06898974]\n",
      "[0.11270833 3.22907296] [1. 1.] [ 1.66360686 30.54517939]\n",
      "[0.11713824 3.1859051 ] [1. 1.] [ 1.65809677 30.31877695]\n",
      "[0.13113025 2.00105075] [1. 1.] [ 1.33371202 24.03498204]\n",
      "[0.13408012 1.85475052] [0.99583333 1.        ] [ 1.32911064 23.90189724]\n",
      "[0.1149795 1.5253094] [1. 1.] [ 1.0189772  18.24400796]\n",
      "[0.11921996 1.27157641] [0.99583333 1.        ] [ 1.02834314 18.2214689 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     initial_ensembles \u001b[38;5;241m=\u001b[39m \u001b[43mget_updated_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles_feats_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdkit_feats_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     G_u_train \u001b[38;5;241m=\u001b[39m get_targets_with_weights(smiles_feats_train, rdkit_feats_train, initial_ensembles, size_ens \u001b[38;5;241m=\u001b[39m size_ens)\n\u001b[1;32m      6\u001b[0m     catch \u001b[38;5;241m=\u001b[39m Parallel(n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)(delayed(inverse_transform)(G_u_train, i)  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(G_u_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[60], line 8\u001b[0m, in \u001b[0;36mget_updated_ensemble\u001b[0;34m(data1, data2, initial_ensembles)\u001b[0m\n\u001b[1;32m      6\u001b[0m all_covs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_covs)\n\u001b[1;32m      7\u001b[0m D_plus_cov \u001b[38;5;241m=\u001b[39m (D \u001b[38;5;241m+\u001b[39m all_covs)\n\u001b[0;32m----> 8\u001b[0m D_plus_cov_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_plus_cov\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m mid_quant \u001b[38;5;241m=\u001b[39m C\u001b[38;5;129m@D_plus_cov_inv\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mid_quant = C@np.linalg.inv(D + lambda_inv)\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/enkf/lib/python3.11/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,10000):\n",
    "    \n",
    "    initial_ensembles = get_updated_ensemble(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    \n",
    "    G_u_train = get_targets_with_weights(smiles_feats_train, rdkit_feats_train, initial_ensembles, size_ens = size_ens)\n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_train, i)  for i in range(G_u_train.shape[0]))\n",
    "    G_u_train = np.array(catch)\n",
    "    \n",
    "    y_train_cur = std_targets.inverse_transform(y_train)\n",
    "    \n",
    "    li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_train = ui_train - li_train\n",
    "    avg_width_train = width_train.mean(0)\n",
    "    \n",
    "    ind_train = (y_train_cur >= li_train) & (y_train_cur <= ui_train)\n",
    "    coverage_train= ind_train.mean(0)\n",
    "    \n",
    "    averaged_targets_train = G_u_train.mean(0)\n",
    "    rmse_train = np.sqrt(((y_train_cur -averaged_targets_train)**2).mean(0))\n",
    "    print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "    G_u_test = get_targets_with_weights(smiles_feats_valid, rdkit_feats_valid, initial_ensembles, size_ens = size_ens)\n",
    "    \n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_test, i)  for i in range(G_u_test.shape[0]))\n",
    "    G_u_test = np.array(catch)\n",
    "    \n",
    "    y_valid_cur = std_targets.inverse_transform(y_valid)    \n",
    "    \n",
    "    li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_test = ui_test - li_test\n",
    "    avg_width_test = width_test.mean(0)\n",
    "    \n",
    "    ind_test = (y_valid_cur >= li_test) & (y_valid_cur <= ui_test)\n",
    "    coverage_test= ind_test.mean(0)\n",
    "    \n",
    "    averaged_targets_test = G_u_test.mean(0)\n",
    "    rmse_test = np.sqrt(((y_valid_cur -averaged_targets_test)**2).mean(0))    \n",
    "    \n",
    "    # plt.scatter(y_valid[:, 0], averaged_targets_test[:,0])\n",
    "    # plt.axline((0,0), slope = 1, c= \"black\")\n",
    "    # plt.show()\n",
    "    # plt.scatter(y_valid[:,1], averaged_targets_test[:, 1])\n",
    "    # plt.axline((0,0), slope = 1, c= \"black\")\n",
    "    # plt.show()\n",
    "    \n",
    "    if coverage_train.mean() < 0.95:\n",
    "        break\n",
    "    \n",
    "    print(rmse_test, coverage_test, avg_width_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eb6c3-e084-4e43-aa5e-3f9b0f5ee016",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_u_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8df43-9ae3-47f6-9c69-ac77c38a6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47207771-df91-48f0-9963-9a78989bcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.sample(range(y_valid_cur.shape[0]), k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f17ea-a47c-421d-bd32-7a0dc0f1af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 2,figsize=(15, 15))\n",
    "# axs = axs.ravel()\n",
    "# counter = 0\n",
    "for idx, i in enumerate(random_idx):\n",
    "    # print(counter)\n",
    "    truth = y_valid_cur[i,:]\n",
    "    preds = G_u_test[:, i,:]\n",
    "    percts = np.percentile(preds, axis = 0, q = (2.5, 97.5))\n",
    "    lis = percts[0,:]\n",
    "    uis = percts[1,:]\n",
    "    \n",
    "    \n",
    "    axs[idx, 0].hist(preds[:,0])\n",
    "    axs[idx, 0].axvline(truth[0], color='green', linewidth=2)\n",
    "    axs[idx, 0].axvline(lis[0], color='red', linewidth=2)\n",
    "    axs[idx, 0].axvline(uis[0], color='red', linewidth=2)\n",
    "    \n",
    "    axs[idx, 1].hist(preds[:,1])\n",
    "    axs[idx, 1].axvline(truth[1], color='green', linewidth=2)\n",
    "    axs[idx, 1].axvline(lis[1], color='red', linewidth=2)\n",
    "    axs[idx, 1].axvline(uis[1], color='red', linewidth=2)\n",
    "    \n",
    "    # counter+=2\n",
    "    # print(counter)\n",
    "    \n",
    "    # plt.show()\n",
    "plt.savefig('prediction_intervals.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230a62b-f36f-4ec0-9c7d-0ba75c9de369",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_valid_cur[:, 0], averaged_targets_test[:,0])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()\n",
    "plt.scatter(y_valid_cur[:,1], averaged_targets_test[:, 1])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
