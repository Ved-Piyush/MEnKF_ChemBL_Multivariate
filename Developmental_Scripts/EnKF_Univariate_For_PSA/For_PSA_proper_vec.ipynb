{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ab5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36084258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 06:06:18.062679: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-05 06:06:18.065745: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-05 06:06:18.108924: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-05 06:06:18.110853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 06:06:18.997625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tf.random.set_seed(seed_value)\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e036b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebaf4ac0-c847-4966-83bd-c3cc1a0bd58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alogp_bottleneck = np.load(\"..//Data/small_mol_phase_3_features_for_psa.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd7d977-20bc-4012-a710-291b706cf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## read in all the drug features\n",
    "# ## for train test and valid\n",
    "train_drugs = alogp_bottleneck[:, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c9e28e-57e1-4989-9377-779597ab9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## read in all the omics features\n",
    "# ## for train test and valid\n",
    "train_omics = alogp_bottleneck[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76624ecc-8bb7-4e52-b95d-1a01cecacb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 10): \n",
    "    input_layer = tf.keras.layers.Input(shape = (X_train_word2vec.shape[1]))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(1)\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8153ea5-cefc-4075-994d-c51461505e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word2vec = train_omics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a7d5f5-e03c-4de4-8bea-d4821cc7209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, h2 = 16,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ff179a-2c64-4bae-bfd5-3ce7f4b40fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 06:06:20.125008: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-05 06:06:20.125053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: c3101.swan.hcc.unl.edu\n",
      "2023-06-05 06:06:20.125062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: c3101.swan.hcc.unl.edu\n",
      "2023-06-05 06:06:20.125187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.85.5\n",
      "2023-06-05 06:06:20.125213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.85.5\n",
      "2023-06-05 06:06:20.125221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.85.5\n"
     ]
    }
   ],
   "source": [
    "ann_15 = ann(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf5144e-59f2-413e-af30-39407819094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                528       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 545\n",
      "Trainable params: 545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann_15.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9c45b5a-9f13-4fd9-b90c-ed94e069b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word2vec = train_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bce5668e-a895-47b4-8f1c-6e5d1fd3c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_20 = ann(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a41f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights_1 = ann_15.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0721ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights_2 = ann_20.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b2ed894",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = total_weights_1 + total_weights_2 + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "999f5e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1091"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb2ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "# batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b49ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a60a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import invgamma, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd888262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3355c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1ce82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7425ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7959ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1db4063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ann_1 = ann_15.get_weights()\n",
    "weights_ann_2 = ann_20.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, batch_data1, initial_ensembles, log_sigma_points_1): \n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "\n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "\n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, 1)\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + 1)].reshape(size_ens, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "\n",
    "    n_hidden_2 = len(weights_ann_2[0].ravel())\n",
    "\n",
    "    initial_ensembles_1 = initial_ensembles.copy()[:, total_weights_1:(total_weights_1+ total_weights_2)]\n",
    "\n",
    "    hidden_weights_2 = initial_ensembles_1[:,:n_hidden_2].reshape(size_ens, batch_data1.shape[1], h2)\n",
    "\n",
    "\n",
    "\n",
    "    hidden_output_2 = np.einsum('ij,kjl->kil', batch_data1, hidden_weights_2)\n",
    "\n",
    "    hidden_layer_bias_2 = initial_ensembles[:,n_hidden_2:(n_hidden_2 + h2)].reshape(size_ens, 1,  h2)\n",
    "\n",
    "    hidden_output_2 = hidden_output_2+ hidden_layer_bias_2\n",
    "\n",
    "    n_pred_weights_2 = len(weights_ann_2[2].ravel())\n",
    "\n",
    "    output_weights_2 = initial_ensembles_1[:,(n_hidden_2 + h2):(n_hidden_2 + h2 + n_pred_weights_2) ].reshape(size_ens, h2, 1)\n",
    "\n",
    "\n",
    "    output_2 = np.einsum('ijk,ikl->ijl', hidden_output_2, output_weights_2)\n",
    "\n",
    "\n",
    "    output_layer_bias_2 = initial_ensembles_1[:,(n_hidden_2 + h2 + n_pred_weights_2):(n_hidden_2 + h2 + n_pred_weights_2 + 1)].reshape(size_ens, 1, 1)\n",
    "\n",
    "\n",
    "    final_output_2 = output_2 + output_layer_bias_2\n",
    "\n",
    "\n",
    "    weights_1 = initial_ensembles[:, :total_weights_1]\n",
    "\n",
    "    weights_2 = initial_ensembles[:, total_weights_1:(total_weights_1 + total_weights_2)]\n",
    "\n",
    "\n",
    "    avg_weights = initial_ensembles[:, -1].reshape(-1,1)\n",
    "\n",
    "    avg_weights_sig = expit(avg_weights)\n",
    "    \n",
    "    avg_weights_sig = avg_weights_sig.reshape(avg_weights_sig.shape[0], 1, avg_weights_sig.shape[1])\n",
    "    \n",
    "    complement_weights_sig = 1 - expit(avg_weights)\n",
    "    \n",
    "    complement_weights_sig = complement_weights_sig.reshape(complement_weights_sig.shape[0], 1, complement_weights_sig.shape[1])\n",
    "\n",
    "    final_output_1 = final_output_1*complement_weights_sig\n",
    "    \n",
    "    final_output_2 = final_output_2*avg_weights_sig\n",
    "    \n",
    "    output_1_ravel = final_output_1.reshape(size_ens, final_output_1.shape[1]*final_output_1.shape[2])\n",
    "\n",
    "    output_2_ravel = final_output_2.reshape(size_ens, final_output_2.shape[1]*final_output_2.shape[2])\n",
    "\n",
    "\n",
    "    output_1_ravel = output_1_ravel\n",
    "\n",
    "    output_2_ravel = output_2_ravel\n",
    "\n",
    "\n",
    "\n",
    "    weights_1_add = np.zeros((size_ens, (total_weights_2 - total_weights_1)))\n",
    "\n",
    "\n",
    "\n",
    "    weights_1 = np.hstack((weights_1, weights_1_add))\n",
    "    \n",
    "\n",
    "\n",
    "    stack_1 = np.hstack((output_1_ravel, weights_1, np.repeat(0, size_ens).reshape(-1,1), np.repeat(0, size_ens).reshape(-1,1)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    stack_2 = np.hstack((output_2_ravel, weights_2, avg_weights, log_sigma_points_1))\n",
    "\n",
    "    \n",
    "    initial_aug_state = np.hstack((stack_1, stack_2)) \n",
    "    \n",
    "\n",
    "    return initial_aug_state , output_1_ravel, output_2_ravel, log_sigma_points_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e3ef6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0eabfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b982682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_weights =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10d77915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2a6de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "471e3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_needed = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63796187",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = shape_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6aee42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = int(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc234219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "23d87ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f5427e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c2dbaec-3249-404a-9971-c2fc17f68356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9cf12935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef051014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "881d1612-f98f-4df6-89b0-90bd2e97de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac0108fa-359e-4d88-ba02-776a6f19b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6cd29e03-d778-4eeb-98b5-de815840cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dd0addc7-a56f-4829-bc5d-86386d1de1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = pd.read_csv(\"..//Data/smiles_with_rdkit_with_small_phase_3_outputs.csv\")\n",
    "y_valid_psa = y_valid.iloc[:,2].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "27357829-fcc0-4d0e-9752-ce56d2377996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "std_targets = pickle.load(open('..//Data//psa_target_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9f43071-1390-4f96-b6ed-2b4216d83b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = std_targets.transform(y_valid_psa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "52152167-9139-4547-ab74-bfac571cf5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.579356024681393"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa106e5e-7f8f-4620-9a16-43194267fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_param = int(y_train.var()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a94cef0-e44d-401e-9813-b4941af4f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_param = int(1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5d9ed18-e108-47ec-b823-22bf0e2b5e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6e4fb25b-9fbc-4aa2-b40d-02752c679e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "15d7ba84-371a-46ce-bd95-a444e1c40dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b0389716-08ec-40aa-a9e7-95150f2f3bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_omics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8d95776e-a5df-4197-8fdb-a97737f192e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_drugs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75c12e9a-898a-45db-9ee2-eb652adfbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_one_real_world(idx, inflation_factor = 0.5, cutoff = 100): \n",
    "    catch_1 = []\n",
    "    catch_2 = []\n",
    "    catch_3 = []\n",
    "    catch_4 = []\n",
    "    catch_5 = []\n",
    "#     from scipy.special import expit\n",
    "    patience_smaller = 0\n",
    "    patience_uns = 0\n",
    "\n",
    "    \n",
    "    best_train_width = 100000\n",
    "    \n",
    "\n",
    "    X_train_logits = y_train.reshape(-1,1)\n",
    "\n",
    "    \n",
    "    ## create training batch chunks\n",
    "    train_idx = list(range(0, X_train_logits.shape[0]))\n",
    "    batch_chunks = [train_idx[i:i+batch_size] for i in range(0,len(train_idx),batch_size)]\n",
    "\n",
    "    ## generate some augmented variable for iteration 0\n",
    "    initial_aug_state_mean = np.repeat(0, total_weights)\n",
    "    initial_aug_state_mean = initial_aug_state_mean.reshape(-1,1)\n",
    "\n",
    "    initial_aug_state_cov = var_weights*np.identity((total_weights))\n",
    "    initial_ensembles = mvn(initial_aug_state_mean.reshape(initial_aug_state_mean.shape[0],), initial_aug_state_cov).rvs(size = size_ens)\n",
    "\n",
    "    exit_iter_no_thresh = 0\n",
    "    log_sigma_points_1 = (np.log(gamma(gamma_param, scale = 1/100).rvs(size_ens))).reshape(size_ens, 1)\n",
    "\n",
    "    train_lstm = train_omics\n",
    "    \n",
    "    train_doc2vec = train_drugs\n",
    "    \n",
    "    \n",
    "    # train_doc2vec = train_omics\n",
    "\n",
    "    train_valid_lstm = train_lstm\n",
    "    train_valid_doc2vec = train_doc2vec\n",
    "\n",
    "    \n",
    "    best_coverage_train = 0\n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    for iter1 in tqdm(range(0,500)):\n",
    "        random_idx = random.sample(range(train_valid_lstm.shape[0]), train_valid_lstm.shape[0])\n",
    "        train_valid_lstm =train_valid_lstm[random_idx, :]\n",
    "        train_valid_doc2vec = train_valid_doc2vec[random_idx, :]\n",
    "        X_train_logits = X_train_logits[random_idx, :]\n",
    "        \n",
    "        for batch_idx in (batch_chunks):\n",
    "\n",
    "            batch_data = train_valid_lstm[batch_idx,:]\n",
    "            batch_data1 = train_valid_doc2vec[batch_idx,:]\n",
    "            # print(batch_data.shape)\n",
    "            batch_targets = X_train_logits[batch_idx,:]\n",
    "            \n",
    "\n",
    "            column_mod_2_shape = total_weights_2 + batch_data.shape[0]*1 + 1 + 1\n",
    "        \n",
    "            H_t = np.hstack((np.identity(batch_targets.shape[0]), np.zeros((batch_targets.shape[0], column_mod_2_shape-batch_targets.shape[0]))))\n",
    "\n",
    "            current_aug_state, column_mod_1, column_mod_2, log_sigma_points_1 = get_targets_with_weights(batch_data, batch_data1, initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "            # print(current_aug_state.shape)\n",
    "            \n",
    "            # var_targets_vec = np.exp(log_sigma_points_1)\n",
    "            var_targets_vec = np.log(1 + np.exp(log_sigma_points_1))\n",
    "            \n",
    "            var_targets_vec = var_targets_vec\n",
    "            \n",
    "            # current_aug_state_var = np.cov(current_aug_state.T) + inflation_factor*np.identity(current_aug_state.shape[1])\n",
    "            \n",
    "            current_aug_state_var = np.cov(current_aug_state.T) \n",
    "            \n",
    "            G_t = np.array([1 , 1]).reshape(-1,1)\n",
    "            \n",
    "            scirpt_H_t = np.kron(G_t.T, H_t)\n",
    "            \n",
    "            temp1 = current_aug_state_var@scirpt_H_t.T\n",
    "            \n",
    "            temp2 = scirpt_H_t@current_aug_state_var@scirpt_H_t.T\n",
    "        \n",
    "            for ensemble_idx in range(0, current_aug_state.shape[0]):\n",
    "                \n",
    "                var_targets1 = var_targets_vec[ensemble_idx,:]\n",
    "                \n",
    "                R_t = var_targets1*np.identity(batch_targets.shape[0])\n",
    "            \n",
    "                measurement_error = mvn(np.repeat(0,batch_targets.shape[0]), var_targets1*np.identity(batch_targets.shape[0])).rvs(1).reshape(-1,1)\n",
    "            \n",
    "                target_current = batch_targets + measurement_error\n",
    "                \n",
    "                # print(target_current.shape)\n",
    "                \n",
    "                K_t = temp1@np.linalg.pinv(temp2 + R_t)\n",
    "                \n",
    "                # print(K_t.shape)\n",
    "\n",
    "                current_aug_state[ensemble_idx,:] = current_aug_state[ensemble_idx,:] +(K_t@(target_current -scirpt_H_t@current_aug_state[ensemble_idx,:].reshape(-1,1))).reshape(current_aug_state.shape[1],)\n",
    "        \n",
    "\n",
    "            weights_ann_1 = current_aug_state[:,batch_targets.shape[0]:(batch_targets.shape[0] + total_weights_1)]      \n",
    "\n",
    "            weights_ann_2 = current_aug_state[:,-(total_weights_2+1):-2]    \n",
    "\n",
    "            initial_ensembles = np.hstack((weights_ann_1, weights_ann_2, current_aug_state[:,-2].reshape(-1,1)))\n",
    "            \n",
    "            log_sigma_points_1 = current_aug_state[:,-1].reshape(-1,1)\n",
    "               \n",
    "            avg_betas = expit(current_aug_state[:,-2])\n",
    "        \n",
    "            complement = 1-avg_betas\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            current_aug_state1, column_mod_11, column_mod_21, log_sigma_points_1 = get_targets_with_weights(train_lstm, train_doc2vec, initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "            initial_targets = column_mod_11 + column_mod_21\n",
    "            \n",
    "            \n",
    "            initial_targets = initial_targets.reshape(size_ens, train_lstm.shape[0],1)\n",
    "            \n",
    "            initial_targets_train = initial_targets\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             ind = (X_train_logits_true >= np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[0,:,:]) & (X_train_logits_true <= np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "        \n",
    "            # initial_targets_softmax = expit(initial_targets)\n",
    "        \n",
    "            # initial_softmax_train = initial_targets_softmax\n",
    "            \n",
    "            initial_targets_train_mean = initial_targets_train.mean(0)\n",
    "            \n",
    "            # print(initial_targets_train_mean.shape)\n",
    "            \n",
    "            li = np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[0,:,:]\n",
    "            \n",
    "            ui = np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "            \n",
    "            width = ui - li\n",
    "            \n",
    "            avg_width_train = np.mean(width)\n",
    "            \n",
    "            # interim = np.vstack((catch_train_probs[idx] , catch_valid_probs[idx] ))\n",
    "        \n",
    "            interim = (X_train_logits)\n",
    "            \n",
    "            # print(interim.shape)\n",
    "            \n",
    "            ind = (interim >= li) & (interim <= ui)\n",
    "            \n",
    "            coverage_train= np.mean(ind.ravel())  \n",
    "            \n",
    "            train_mae = np.sqrt(mean_squared_error(interim, initial_targets_train_mean))\n",
    "            \n",
    "            print(train_mae, coverage_train, avg_width_train)\n",
    "        \n",
    "#         plt.scatter(interim, initial_targets_train_mean)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4dcabe4b-76ae-4302-8ada-218067cc7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d2a0a04c-eb2b-4dd6-938a-0558970523ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e9c9c9120f450fa3454c985e3a7a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.95540420695977 1.0 48.0359053585273\n",
      "4.859681869720132 1.0 47.09130929761016\n",
      "4.452042225240546 1.0 45.190208824073245\n",
      "4.437492127281723 1.0 44.73946350530608\n",
      "4.575758919923841 1.0 43.634613100807265\n",
      "3.8821666384157947 1.0 42.496923634407395\n",
      "3.9684868013037877 1.0 42.55773341121317\n",
      "3.968460341912967 0.9989572471324296 42.98260289726499\n",
      "3.8956689950996557 1.0 42.49522684904383\n",
      "3.8673784168449643 0.9989572471324296 41.848615002073345\n",
      "4.247626273464145 1.0 42.119341275162654\n",
      "4.2233749006986 1.0 41.33329342906157\n",
      "4.437268761523949 0.9989572471324296 41.21195813094248\n",
      "4.190619544602207 0.9989572471324296 41.16561007943307\n",
      "4.058951885492051 1.0 41.07033625107991\n",
      "3.953797677441995 1.0 41.08262803113757\n",
      "4.192227956866442 1.0 41.66897309445307\n",
      "3.854946039144186 1.0 41.414402282619776\n",
      "4.056327206494764 0.9989572471324296 41.50857494969731\n",
      "4.1039617370252275 1.0 41.8754013849405\n",
      "4.1492437879750375 1.0 41.52205180989871\n",
      "4.193037570029918 1.0 41.83739775976431\n",
      "4.157208997841464 1.0 41.63774195775848\n",
      "4.108287665937925 1.0 41.68260643210472\n",
      "4.142646473237655 1.0 41.326737424903705\n",
      "4.132039938846515 1.0 41.35925141640039\n",
      "4.25007802068901 0.9989572471324296 40.761685637639914\n",
      "4.133103270560575 1.0 41.11684040200599\n",
      "4.298137782006719 0.9989572471324296 40.92846240751743\n",
      "4.055403041735968 0.9989572471324296 41.02657483499051\n",
      "4.156011222240706 0.9989572471324296 41.23011961693103\n",
      "4.248614336933473 0.9989572471324296 41.86219677196584\n",
      "4.018538707935005 0.9989572471324296 41.34637107425797\n",
      "4.014959523555319 0.9989572471324296 41.945598059271404\n",
      "4.066079713710368 0.9989572471324296 41.365664742797186\n",
      "4.268468215099582 0.9989572471324296 41.52283713569565\n",
      "4.248668812793483 0.9989572471324296 41.604010743255586\n",
      "4.159960576676172 0.9989572471324296 41.57636290762905\n",
      "4.236446671614675 0.9989572471324296 41.37455782072645\n",
      "4.132530692898941 0.9989572471324296 41.269930373807846\n",
      "4.064471936937472 0.9989572471324296 41.31988083761524\n",
      "3.975826319084441 0.9989572471324296 41.10144267770362\n",
      "4.089003776232966 1.0 41.26568140976254\n",
      "4.22220774943346 0.9989572471324296 40.83334131204895\n",
      "4.178749559609593 1.0 40.83333432735297\n",
      "4.249881240602885 0.9989572471324296 40.58448449377243\n",
      "4.078273999357169 0.9989572471324296 40.96483205068152\n",
      "3.9877113524156123 0.9989572471324296 40.989997046527\n",
      "3.9502368696533208 0.9989572471324296 41.2492280102082\n",
      "3.8262208875422665 0.9989572471324296 41.05683389816908\n",
      "4.055532254832201 0.9989572471324296 41.084444802950486\n",
      "3.870091322104246 1.0 41.69285309445278\n",
      "3.9255722037657104 0.9989572471324296 41.97777442090113\n",
      "4.120553003028383 1.0 42.15050639294464\n",
      "3.990023872769038 0.9989572471324296 41.29881070513063\n",
      "4.194208600406628 1.0 42.42400556577446\n",
      "4.385318688230721 0.9989572471324296 40.39193510781957\n",
      "4.240980604982061 1.0 40.06927487946401\n",
      "4.291680429062908 0.9989572471324296 39.68536903988637\n",
      "4.381615079071123 0.9989572471324296 39.77672842080266\n",
      "4.665490174760821 0.9989572471324296 39.20074651175788\n",
      "4.659902077330671 0.9989572471324296 39.36931030156873\n",
      "4.67748558323646 0.9989572471324296 39.059433617998614\n",
      "4.561713430664784 0.9989572471324296 39.76358201072818\n",
      "4.112155160915585 0.9989572471324296 39.5898111462294\n",
      "4.5955683015373685 0.9989572471324296 39.80573752252082\n",
      "4.586683261112039 0.9989572471324296 39.4203251980051\n",
      "4.808788978270149 0.9989572471324296 39.487192995003674\n",
      "4.932071204133942 0.9989572471324296 39.17041769934017\n",
      "4.8505277423296524 0.9989572471324296 39.44727485250038\n",
      "4.816539966459528 0.9989572471324296 39.16800865470006\n",
      "4.581811338763656 0.9989572471324296 39.21281320164057\n",
      "4.758272106316308 0.9989572471324296 38.659419568679596\n",
      "4.65585216624093 0.9989572471324296 38.839381023475454\n",
      "4.760110896753225 0.9989572471324296 38.064051588455534\n",
      "4.269191338883099 0.9989572471324296 38.215455366619274\n",
      "4.328801120623362 0.9989572471324296 37.963128267740764\n",
      "4.087143148802613 0.9989572471324296 37.58720809015109\n",
      "4.084943744444155 0.9989572471324296 37.40327618784275\n",
      "4.1152836307623355 0.9989572471324296 37.42579864780701\n",
      "4.1501575902845875 0.9989572471324296 37.40597036305473\n",
      "4.082748056169763 0.9989572471324296 37.06921793603106\n",
      "4.1608119657743226 0.9989572471324296 37.25402390761925\n",
      "4.255085096253105 0.9989572471324296 37.17334949195767\n",
      "4.13418217469314 0.9989572471324296 37.20256640214802\n",
      "3.9937277926603514 0.9989572471324296 37.07707075035312\n",
      "4.156925607026714 0.9989572471324296 37.209553675243946\n",
      "4.117148969030711 0.9979144942648592 37.11450651079962\n",
      "4.087900510934845 0.9989572471324296 37.12077918661053\n",
      "4.125109658929372 0.9979144942648592 36.79132576937979\n",
      "4.266340455315625 1.0 36.944872260734655\n",
      "4.227021927791767 0.9989572471324296 36.897178292524856\n",
      "4.330581300374744 1.0 36.88815722343769\n",
      "4.447910592870971 1.0 36.843659645810746\n",
      "4.311484972396716 1.0 36.73626495550979\n",
      "4.272206708512458 0.9989572471324296 36.84080885497053\n",
      "4.218878154948956 0.9989572471324296 36.820164509290755\n",
      "4.308899968723268 0.9989572471324296 37.0670899727475\n",
      "4.334151454822456 1.0 37.05187936759768\n",
      "4.361688260617312 0.9989572471324296 37.13086184539973\n",
      "4.408198971487529 0.9989572471324296 37.11730501209229\n",
      "4.434594779348778 0.9989572471324296 37.17610999477362\n",
      "4.515860555985335 0.9989572471324296 37.25864389460154\n",
      "4.5136400152097504 1.0 37.29071042629006\n",
      "4.6002828859604525 1.0 37.25674310644716\n",
      "4.486183374903289 1.0 37.3294830177204\n",
      "4.426044183816994 1.0 37.31634166760545\n",
      "4.414083000206103 1.0 37.1886235311988\n",
      "4.468417724640858 1.0 37.233665347234954\n",
      "4.493776099611829 1.0 37.256804014780805\n",
      "4.426619941543453 1.0 37.36039971456967\n",
      "4.607229953622266 1.0 37.440099378947345\n",
      "4.568130465310019 1.0 37.380071115814204\n",
      "4.641342774599015 1.0 37.27181877012534\n",
      "4.5113392927034655 1.0 37.510693029385685\n",
      "4.569105647956346 1.0 37.04920337582916\n",
      "4.400893723095626 1.0 37.17829099083215\n",
      "4.555464586036002 1.0 36.703403145862026\n",
      "4.383836454083257 1.0 36.63023582599196\n",
      "4.323263332234685 1.0 36.432593597780155\n",
      "4.5311616697128505 0.9979144942648592 36.61604197693247\n",
      "4.517043093675414 0.9979144942648592 36.23207179388689\n",
      "4.472318684328691 0.9979144942648592 36.41373712914177\n",
      "4.392604054818194 0.9979144942648592 35.929245603756385\n",
      "4.31974125934485 0.9979144942648592 36.306563344665804\n",
      "4.240098440425333 0.9979144942648592 36.1207286695526\n",
      "4.3567520738236984 0.9979144942648592 36.00284301498506\n",
      "4.23277805303075 0.9979144942648592 35.86235654477552\n",
      "4.1380216706090405 0.9979144942648592 36.03784988392703\n",
      "4.065993862533051 0.9979144942648592 36.114186948746735\n",
      "4.095571984507366 0.9979144942648592 36.24684087309906\n",
      "4.215077271775837 0.9979144942648592 36.27853748790646\n",
      "4.215465641405599 0.9979144942648592 36.41101103451924\n",
      "4.213777944769195 0.9979144942648592 36.283697654328186\n",
      "4.317253869884489 0.9979144942648592 36.20991719057134\n",
      "4.2181287496328395 0.9979144942648592 36.10807377572276\n",
      "4.123324716767883 0.9979144942648592 36.2142975109047\n",
      "4.145462570777104 0.9979144942648592 36.23038685065253\n",
      "4.188333445831889 0.9979144942648592 36.20114350715682\n",
      "4.284021359974818 0.9979144942648592 36.426585471813546\n",
      "4.253132227558497 0.9979144942648592 36.361187568619336\n",
      "4.204253299603794 0.9979144942648592 36.21853817316656\n",
      "4.160515230065871 0.9979144942648592 36.28803098957204\n",
      "4.246327504683617 0.9979144942648592 36.33536904393409\n",
      "4.290440567511326 0.9979144942648592 36.22284723249591\n",
      "4.178174515254042 0.9979144942648592 36.33398948730794\n",
      "4.216876351665781 0.9979144942648592 36.276372888235485\n",
      "4.194686416726435 0.9979144942648592 36.276137093871604\n",
      "4.16737955735204 0.9979144942648592 36.25820026309396\n",
      "4.064561417859398 0.9979144942648592 36.25602149579187\n",
      "4.167392583645382 0.9989572471324296 36.27301249211536\n",
      "4.0465266657765655 1.0 36.24284642458255\n",
      "4.241421983661217 0.9989572471324296 36.28294310113478\n",
      "4.116153363434259 1.0 35.981998919898\n",
      "4.16079117333535 1.0 36.00932147728529\n",
      "4.076688715909913 1.0 35.88772918080463\n",
      "4.196592655222163 1.0 35.94578793701716\n",
      "4.159553006354115 1.0 36.00052175750005\n",
      "4.14545071313052 0.9989572471324296 36.001766476658695\n",
      "4.089501030755999 1.0 36.07408446639669\n",
      "4.1028931598471585 1.0 36.098274576856944\n",
      "4.150812616428821 1.0 36.03407318647873\n",
      "4.167718158428321 0.9989572471324296 36.003327910393374\n",
      "4.11647971063095 0.9989572471324296 36.08973133161006\n",
      "4.051250849952932 0.9989572471324296 36.083310550456346\n",
      "4.075570001643379 0.9989572471324296 36.00169424575485\n",
      "4.185119036241089 0.9989572471324296 35.95412023103678\n",
      "4.156745749676751 0.9989572471324296 36.02065668042044\n",
      "4.431513846081691 0.9989572471324296 35.79002822804108\n",
      "4.13478613768683 0.9989572471324296 35.83355643561287\n",
      "4.12459064699339 0.9989572471324296 35.81300313063442\n",
      "4.177898179230246 0.9989572471324296 35.820758458827825\n",
      "4.149887369743165 0.9989572471324296 35.82106146492013\n",
      "4.178264912115573 0.9989572471324296 35.71759326567836\n",
      "4.217522585492111 0.9989572471324296 35.70478838219321\n",
      "4.173071745833889 0.9989572471324296 35.53680852016407\n",
      "4.1842854021149885 0.9989572471324296 35.35042448880416\n",
      "4.2164595981836035 0.9989572471324296 35.39378400630264\n",
      "4.2928095316420825 0.9989572471324296 35.361748728091854\n",
      "4.33760333859723 0.9989572471324296 35.23295013508515\n",
      "4.382536943042721 0.9979144942648592 34.89648471751972\n",
      "4.308400405526066 0.9979144942648592 34.865240468611894\n",
      "4.3476813245534 0.9979144942648592 34.86757675766692\n",
      "4.304708643536992 0.9979144942648592 35.00396059118658\n",
      "4.271278019061554 0.9968717413972888 35.10260408719719\n",
      "4.30374498708168 0.9968717413972888 35.174006719241696\n",
      "4.253065048411391 0.9968717413972888 35.05837470077314\n",
      "4.275355426946823 0.9968717413972888 34.92415403261148\n",
      "4.288214357822173 0.9968717413972888 34.913042651918396\n",
      "4.295959464183328 0.9968717413972888 35.01262261401169\n",
      "4.224132994157557 0.9968717413972888 34.954294283974995\n",
      "4.226362916480028 0.9968717413972888 34.89224805632992\n",
      "4.194534688413784 0.9968717413972888 34.980601444116054\n",
      "4.176530204717059 0.9968717413972888 34.957179069188655\n",
      "4.166135497985683 0.9968717413972888 34.92255316209025\n",
      "4.100894926003086 0.9968717413972888 34.94947553986288\n",
      "4.0938429086537145 0.9968717413972888 35.03896680128311\n",
      "4.094380600359865 0.9968717413972888 35.14322579462536\n",
      "4.087615629036239 0.9979144942648592 35.27232587402685\n",
      "4.09512169387488 0.9989572471324296 35.16920695379936\n",
      "4.019353888411147 0.9989572471324296 35.15944076908649\n",
      "4.089872209426517 0.9979144942648592 35.16689963026001\n",
      "4.049498081165715 0.9979144942648592 35.21752185333233\n",
      "3.970476699899962 0.9979144942648592 35.32467540811717\n",
      "3.951663419729255 0.9979144942648592 35.318660663562305\n",
      "3.957035198092543 0.9979144942648592 35.36730468414362\n",
      "4.001788547745461 0.9979144942648592 35.23694756495527\n",
      "4.043246134474146 0.9979144942648592 35.33769542471668\n",
      "4.067965272580088 0.9979144942648592 35.37559955760247\n",
      "4.024660960640195 0.9979144942648592 35.39640680747548\n",
      "4.165571033485723 0.9989572471324296 35.43343249425747\n",
      "4.249792344087646 0.9989572471324296 35.4438046759391\n",
      "4.232369808275541 0.9989572471324296 35.28059517192506\n",
      "4.160092983221001 0.9989572471324296 35.10266851815677\n",
      "4.148656148214808 0.9989572471324296 34.95665853386561\n",
      "4.247112677091807 0.9989572471324296 34.98577032053152\n",
      "4.166463357246955 0.9989572471324296 34.94611599042069\n",
      "4.23495416574695 0.9989572471324296 34.77549336065816\n",
      "4.22638204290262 0.9989572471324296 34.783913687555874\n",
      "4.22368068650967 0.9989572471324296 34.67354391439428\n",
      "4.165009760864956 0.9989572471324296 34.87622134286891\n",
      "4.049789631083239 0.9989572471324296 34.621226528711205\n",
      "3.945419092433083 0.9989572471324296 35.021866886421286\n",
      "4.040454623132841 0.9989572471324296 34.48748059261806\n",
      "4.226031628346324 0.9989572471324296 34.858274673171884\n",
      "3.9845438638652815 0.9989572471324296 34.141891706380676\n",
      "3.951443314316734 0.9989572471324296 34.61490814396885\n",
      "3.862463917701347 0.9989572471324296 34.36928330103352\n",
      "3.7577029003097344 0.9989572471324296 34.32238343238393\n",
      "3.8710052392138223 0.9989572471324296 34.41179082412621\n",
      "3.8267461052987706 0.9989572471324296 34.43040215454863\n",
      "3.754032003847805 0.9989572471324296 34.251624008363976\n",
      "3.7797853209624956 0.9989572471324296 34.187373596467275\n",
      "3.777795955671506 0.9989572471324296 34.17062306355704\n",
      "3.7989659885610023 0.9989572471324296 34.11662816630659\n",
      "3.783842271799366 0.9989572471324296 34.2832116765018\n",
      "3.7460459337207337 0.9989572471324296 34.21944343936216\n",
      "3.66227134027449 0.9989572471324296 34.23187846799791\n",
      "3.7167832254997566 0.9989572471324296 34.19715627303716\n",
      "3.758501638259733 0.9989572471324296 34.07913191729407\n",
      "3.750180234573439 0.9979144942648592 34.143162141100184\n",
      "3.6756324567210563 0.9979144942648592 34.12555069633532\n",
      "3.757694529169753 0.9979144942648592 34.12507461549197\n",
      "3.756818698106107 0.9979144942648592 34.12150571035213\n",
      "3.7633537407828905 0.9979144942648592 34.02979542837233\n",
      "3.7485886157349304 0.9979144942648592 33.99140307699174\n",
      "3.7801168474908375 0.9979144942648592 33.85870282906475\n",
      "3.8272222594398975 0.9979144942648592 33.88268920738238\n",
      "3.8668070022610737 0.9979144942648592 33.89226190019704\n",
      "3.920166726156319 0.9979144942648592 33.91597163086433\n",
      "3.864037309476561 0.9979144942648592 33.883832109655174\n",
      "3.9406453926391225 0.9979144942648592 33.76585428663706\n",
      "3.9544528613203105 0.9979144942648592 33.78189879052099\n",
      "3.9986788733665826 0.9979144942648592 33.65771134853941\n",
      "3.992789050539173 0.9979144942648592 33.69461488809733\n",
      "3.9970128927931734 0.9979144942648592 33.65426193597226\n",
      "4.0168046854644945 0.9979144942648592 33.51206654152951\n",
      "4.062677035960931 0.9979144942648592 33.50202826563114\n",
      "4.015008849710951 0.9979144942648592 33.574308077810684\n",
      "3.8726747998831708 0.9979144942648592 33.48051714771346\n",
      "3.858952555194313 0.9979144942648592 33.57135124781654\n",
      "3.9865744442041375 0.9979144942648592 33.77613171377148\n",
      "3.907431513067111 0.9979144942648592 33.881836460024225\n",
      "3.8247097157332752 0.9979144942648592 33.91198592983094\n",
      "3.7654485222629233 0.9979144942648592 33.933184858490804\n",
      "3.7251876056335735 0.9979144942648592 33.92347815423075\n",
      "3.7476025302986264 0.9979144942648592 33.89014794704523\n",
      "3.709803535232628 0.9979144942648592 33.95780936140476\n",
      "3.7261027837650973 0.9979144942648592 33.900040510095735\n",
      "3.7406280388859403 0.9979144942648592 33.91364585140787\n",
      "3.6405794940762006 0.9979144942648592 33.84800687243341\n",
      "3.579510089869104 0.9979144942648592 33.90225984879994\n",
      "3.5124436572273328 0.9979144942648592 34.06979107163379\n",
      "3.475048033822058 0.9979144942648592 33.99513640727658\n",
      "3.5301055846136817 0.9979144942648592 33.993209763661575\n",
      "3.562564565674681 0.9979144942648592 34.030902101823465\n",
      "3.557448713763542 0.9979144942648592 33.948654154360376\n",
      "3.613435560874196 0.9989572471324296 33.9990272632722\n",
      "3.4949847769028866 0.9979144942648592 33.973285101485786\n",
      "3.505272082599376 0.9979144942648592 34.000378504779064\n",
      "3.5179921743429663 0.9979144942648592 33.95268020206381\n",
      "3.580182277661303 0.9979144942648592 33.864625672244756\n",
      "3.619724304151621 0.9979144942648592 33.922449908206204\n",
      "3.6519232767015466 0.9979144942648592 33.8892176573549\n",
      "3.624810552082075 0.9979144942648592 33.89594923954845\n",
      "3.6570372205630597 0.9979144942648592 33.895677714275294\n",
      "3.6455533044007398 0.9979144942648592 33.847785570327495\n",
      "3.677704777819033 0.9979144942648592 33.823683520437164\n",
      "3.7168107085829933 0.9979144942648592 33.87364502447933\n",
      "3.7267513733295736 0.9979144942648592 33.84804156493121\n",
      "3.6933917418208715 0.9979144942648592 33.879477744486444\n",
      "3.709334516354034 0.9979144942648592 33.85151812428023\n",
      "3.694267939569205 0.9979144942648592 33.78009093954093\n",
      "3.6834250223745326 0.9979144942648592 33.76461166656257\n",
      "3.7161300820885224 0.9979144942648592 33.78715218788903\n",
      "3.6656209629828074 0.9979144942648592 33.696141139979105\n",
      "3.6785293809082384 0.9979144942648592 33.67517917614176\n",
      "3.6489583394163794 0.9979144942648592 33.72618200132457\n",
      "3.640547701793528 0.9979144942648592 33.8498343449551\n",
      "3.598038730519704 0.9979144942648592 33.796851061500355\n",
      "3.596687113394155 0.9979144942648592 33.85855528023027\n",
      "3.6015393748474844 0.9979144942648592 33.829304208430244\n",
      "3.5818512899936885 0.9979144942648592 33.873756015258884\n",
      "3.578141498850433 0.9979144942648592 33.93834610488593\n",
      "3.579497399187676 0.9979144942648592 33.841493063576465\n",
      "3.5492074340101287 0.9979144942648592 33.84685128184752\n",
      "3.574056745442731 0.9979144942648592 33.89277632979169\n",
      "3.5981158818715544 0.9979144942648592 33.85903188873881\n",
      "3.5966394854567985 0.9979144942648592 33.828998272225206\n",
      "3.637236899147363 0.9979144942648592 33.77583591778745\n",
      "3.6126766084533943 0.9979144942648592 33.74555897772389\n",
      "3.640520051289428 0.9979144942648592 33.80165680217276\n",
      "3.622084237319113 0.9979144942648592 33.85057349670988\n",
      "3.6450929841964532 0.9979144942648592 33.800671817584444\n",
      "3.6450469898480344 0.9979144942648592 33.78354517286395\n",
      "3.638316257788689 0.9979144942648592 33.72333246101309\n",
      "3.6350507599873167 0.9979144942648592 33.670039428042685\n",
      "3.6077807448686845 0.9979144942648592 33.65134007879739\n",
      "3.620880986064092 0.9979144942648592 33.62719471414318\n",
      "3.5961777578814207 0.9979144942648592 33.5764616675115\n",
      "3.619504907814566 0.9979144942648592 33.53951656608908\n",
      "3.591232983551024 0.9979144942648592 33.52141730646229\n",
      "3.5944825098011175 0.9979144942648592 33.51465089986205\n",
      "3.576569468005552 0.9979144942648592 33.71691630916467\n",
      "3.5489069162690434 0.9979144942648592 33.64426191351889\n",
      "3.586556663042904 0.9979144942648592 33.64462304537353\n",
      "3.5858680823993554 0.9979144942648592 33.56206687543057\n",
      "3.565656773658069 0.9979144942648592 33.524672535006694\n",
      "3.5386028591334364 0.9979144942648592 33.516014251071994\n",
      "3.547517508893302 0.9979144942648592 33.60986683287996\n",
      "3.510491894872038 0.9968717413972888 33.62509746090398\n",
      "3.5186277359442513 0.9968717413972888 33.56709718434436\n",
      "3.5590616799998007 0.9968717413972888 33.65330787678669\n",
      "3.5287988441584726 0.9968717413972888 33.63703653003702\n",
      "3.5466043700832466 0.9968717413972888 33.63125821968756\n",
      "3.5270013367008772 0.9968717413972888 33.62583048035518\n",
      "3.536795593590905 0.9968717413972888 33.664514269954466\n",
      "3.532977048967383 0.9968717413972888 33.70471034224763\n",
      "3.5382423336253503 0.9968717413972888 33.69879182006057\n",
      "3.543742016535419 0.9968717413972888 33.72002635873599\n",
      "3.545498883025755 0.9968717413972888 33.71849464886843\n",
      "3.5145062804064833 0.9968717413972888 33.655355402526325\n",
      "3.5357679914797497 0.9968717413972888 33.66852520611038\n",
      "3.5305210247606214 0.9968717413972888 33.713688064043716\n",
      "3.5196431846483174 0.9968717413972888 33.722714863935906\n",
      "3.513754772733312 0.9968717413972888 33.63200152028487\n",
      "3.530605993313141 0.9968717413972888 33.65873296636018\n",
      "3.533318047003005 0.9968717413972888 33.64518522484391\n",
      "3.522646930767037 0.9968717413972888 33.51020401750603\n",
      "3.5065987757074955 0.9968717413972888 33.474067280170864\n",
      "3.4929575281404985 0.9968717413972888 33.47069514627509\n",
      "3.4922762122698745 0.9968717413972888 33.45549894347139\n",
      "3.4907414732445856 0.9968717413972888 33.3831242822765\n",
      "3.4650365503141956 0.9968717413972888 33.24985436298814\n",
      "3.464569580836414 0.9968717413972888 33.284721649231045\n",
      "3.4640177862593253 0.9968717413972888 33.30307382894814\n",
      "3.483213001314067 0.9968717413972888 33.22967408053175\n",
      "3.479202307848813 0.9968717413972888 33.2371136523297\n",
      "3.2350591147122922 0.9968717413972888 33.371230798739056\n",
      "3.1748624474016207 0.9968717413972888 33.33203510368479\n",
      "3.15795241543745 0.9958289885297185 33.28520513615721\n",
      "3.170773386799435 0.9958289885297185 33.274270344339804\n",
      "3.1323101042537953 0.9958289885297185 33.23813792850569\n",
      "3.1412600538155337 0.9958289885297185 33.19224600162843\n",
      "3.13481149686215 0.9958289885297185 33.2476376440734\n",
      "3.149586416618852 0.9958289885297185 33.21936774528886\n",
      "3.1538467789064275 0.9958289885297185 33.19831990499776\n",
      "3.1458053471290652 0.9958289885297185 33.17107207931993\n",
      "3.187817270535128 0.9958289885297185 33.22360758264879\n",
      "3.1867312134443933 0.9958289885297185 33.339306563587925\n",
      "3.0405048707793965 0.9958289885297185 33.31284422069724\n",
      "3.093083162177095 0.9958289885297185 33.333440979756226\n",
      "3.083745712343693 0.9958289885297185 33.36174547325346\n",
      "3.0563245725144 0.9958289885297185 33.41405267979693\n",
      "3.0518790589650844 0.9958289885297185 33.53687147160946\n",
      "3.0577420017074557 0.9958289885297185 33.60568874385487\n",
      "3.0738489494198666 0.9958289885297185 33.604307747627864\n",
      "3.053774910232305 0.9958289885297185 33.56737081191812\n",
      "3.075734422281499 0.9958289885297185 33.67731676792518\n",
      "3.0740160348792758 0.9958289885297185 33.694515609933454\n",
      "3.0699107041899594 0.9958289885297185 33.62401480903575\n",
      "3.06271493986608 0.9958289885297185 33.61905045421625\n",
      "3.073838644057204 0.9958289885297185 33.64889804437151\n",
      "3.094389211066208 0.9958289885297185 33.62995206316181\n",
      "3.0812986121579002 0.9958289885297185 33.61390436844078\n",
      "3.057367787859613 0.9958289885297185 33.528266758560505\n",
      "3.07552887296865 0.9958289885297185 33.51094731109043\n",
      "3.087175354845703 0.9958289885297185 33.476164413699955\n",
      "3.0373212988823473 0.9958289885297185 33.405467989447025\n",
      "3.058076647996134 0.9958289885297185 33.42383547723921\n",
      "3.085230857230808 0.9958289885297185 33.392931028715005\n",
      "3.091281722085004 0.9958289885297185 33.36204492234796\n",
      "3.0870662735649135 0.9958289885297185 33.27842237624103\n",
      "3.0820716348729165 0.9958289885297185 33.307811045430526\n",
      "3.085608614962537 0.9958289885297185 33.202185400694326\n",
      "3.0996524123695246 0.9958289885297185 33.14509573300932\n",
      "3.107829560889231 0.9958289885297185 33.2012498181433\n",
      "3.0928197311168546 0.9958289885297185 33.194732317320394\n",
      "3.101035296474723 0.9958289885297185 33.18431298165163\n",
      "3.1252197025784962 0.9958289885297185 33.12638836772296\n",
      "3.1537172801376916 0.9958289885297185 33.16479139555558\n",
      "3.1284287818373957 0.9958289885297185 33.20242123861723\n",
      "3.1166870109793012 0.9958289885297185 33.1676895680413\n",
      "3.209944660034597 0.9958289885297185 33.403225964386344\n",
      "3.230909479375666 0.9958289885297185 33.38399878284693\n",
      "3.2440036817973064 0.9958289885297185 33.27370929757642\n",
      "3.2317222483675105 0.9958289885297185 33.32979173175093\n",
      "3.2456620521872597 0.9958289885297185 33.32248400324283\n",
      "3.2143527924025004 0.9958289885297185 33.32452131852609\n",
      "3.2092355707580453 0.9958289885297185 33.28965054061877\n",
      "3.1931514611928447 0.9958289885297185 33.322489280006394\n",
      "3.2079445156683972 0.9958289885297185 33.30874804206062\n",
      "3.2035673987634863 0.9958289885297185 33.238153550252186\n",
      "3.1777323400948547 0.9958289885297185 33.25170984536544\n",
      "3.1366277474269793 0.9958289885297185 33.224578799946805\n",
      "3.1262609427742714 0.9958289885297185 33.262630747989924\n",
      "3.1235762066924777 0.9958289885297185 33.23838264294414\n",
      "3.1492359492505684 0.9958289885297185 33.19752965185789\n",
      "3.1576752132373693 0.9958289885297185 33.236047959213536\n",
      "3.1454755358737847 0.9958289885297185 33.20330553383438\n",
      "3.16596675294433 0.9958289885297185 33.16976101640635\n",
      "3.1690353350372784 0.9968717413972888 33.14499080556218\n",
      "3.1770603619744735 0.9968717413972888 33.093548585692666\n",
      "3.1541313315649693 0.9968717413972888 33.10506391681594\n",
      "3.1407413873265146 0.9968717413972888 33.06037495973392\n",
      "3.1380788139333182 0.9968717413972888 33.08686743303813\n",
      "3.1370499346536276 0.9968717413972888 33.07829291129062\n",
      "3.113108073499037 0.9968717413972888 33.084460976408025\n",
      "3.0435419319443233 0.9958289885297185 33.01919943728573\n",
      "3.0551179670303155 0.9958289885297185 33.03997095488429\n",
      "3.0925031753756724 0.9958289885297185 33.077267999637044\n",
      "3.0485693614899274 0.9958289885297185 33.03482110414826\n",
      "3.041582433683545 0.9958289885297185 33.039720825207056\n",
      "3.0224267217634666 0.9958289885297185 33.059629155313736\n",
      "2.9990890925340707 0.9958289885297185 33.03264106113724\n",
      "3.024400763966606 0.9958289885297185 33.075201284740125\n",
      "3.042144030391716 0.9958289885297185 33.104078179114076\n",
      "3.027159003195927 0.9958289885297185 33.1528282796685\n",
      "3.0397023140108668 0.9958289885297185 33.15677502659334\n",
      "3.051967267849852 0.9958289885297185 33.13032435179595\n",
      "3.0352459222362076 0.9958289885297185 33.09559458155794\n",
      "3.110101182316211 0.9958289885297185 33.09284504092992\n",
      "3.1133801764218423 0.9958289885297185 33.116474839363796\n",
      "3.1208711864050107 0.9958289885297185 33.12205612534384\n",
      "3.1251496271234385 0.9958289885297185 33.07390283097867\n",
      "3.1420345536077483 0.9958289885297185 33.08124477544722\n",
      "3.1359802343641174 0.9958289885297185 33.07248733993437\n",
      "3.1628845644119905 0.9958289885297185 33.131508476893906\n",
      "3.1567844711427107 0.9958289885297185 33.165644669840795\n",
      "3.1889907706216905 0.9958289885297185 33.20294239397281\n",
      "3.1597486152778966 0.9968717413972888 33.21335881398894\n",
      "3.1654024638244853 0.9968717413972888 33.20484867258278\n",
      "3.1665122408645803 0.9968717413972888 33.20237645746041\n",
      "3.189562224887531 0.9968717413972888 33.19737573051309\n",
      "3.202747305710525 0.9968717413972888 33.1708607979098\n",
      "3.1830027792797764 0.9968717413972888 33.1951996819378\n",
      "3.2073993172467308 0.9968717413972888 33.20111464488743\n",
      "3.1954622148772116 0.9968717413972888 33.183096914174186\n",
      "3.1872364470435977 0.9968717413972888 33.167549516814894\n",
      "3.1791758822960112 0.9968717413972888 33.16370845241664\n",
      "3.1845504642600604 0.9968717413972888 33.21915263519651\n",
      "3.1961056425420478 0.9968717413972888 33.21413868732266\n",
      "3.2018056446177905 0.9968717413972888 33.210920530382516\n",
      "3.19393904580275 0.9968717413972888 33.17994806574169\n",
      "3.1514127498885176 0.9968717413972888 33.1773367066224\n",
      "3.0690879591911613 0.9968717413972888 33.174140570279654\n",
      "3.099515779754787 0.9968717413972888 33.141036558197854\n",
      "3.095059431300387 0.9968717413972888 33.125740349470504\n",
      "3.088188256180459 0.9968717413972888 33.1431863251069\n",
      "3.090363432232127 0.9968717413972888 33.12125376741971\n",
      "3.108910955774273 0.9968717413972888 33.15380007647746\n",
      "3.136472208022023 0.9968717413972888 33.13045539424474\n",
      "3.127727112925061 0.9968717413972888 33.13940883118147\n",
      "3.1446875646773407 0.9968717413972888 33.13044713362168\n",
      "3.1396139815340702 0.9968717413972888 33.176266377968204\n",
      "3.159327452165087 0.9968717413972888 33.17769161852752\n",
      "3.155982390858194 0.9968717413972888 33.18022436390013\n",
      "3.1687093699750353 0.9968717413972888 33.1598694892251\n",
      "3.175051698013061 0.9968717413972888 33.141352512910906\n",
      "3.171190478440776 0.9968717413972888 33.16268274377557\n",
      "3.1574919681874283 0.9989572471324296 33.21439060057358\n",
      "3.178909345277283 0.9979144942648592 33.18745942784314\n",
      "3.1724731974003335 0.9979144942648592 33.18640300181414\n",
      "3.1778877091859354 0.9979144942648592 33.19108185689744\n",
      "3.1803617436248497 0.9979144942648592 33.2035064362794\n",
      "3.177893689703113 0.9979144942648592 33.21662426980378\n",
      "3.163089560111119 0.9979144942648592 33.213881727101636\n",
      "3.151447865413592 0.9979144942648592 33.217987552807195\n",
      "3.1492178639040547 0.9979144942648592 33.23696071654551\n",
      "3.137961718486302 0.9979144942648592 33.22475928914655\n",
      "3.137222755027079 0.9979144942648592 33.21568055163467\n",
      "3.1307117773503634 0.9979144942648592 33.21991204418522\n",
      "3.142990064870447 0.9979144942648592 33.22386195522254\n",
      "3.149994364035638 0.9979144942648592 33.198060783194606\n",
      "3.137780228645081 0.9979144942648592 33.18452383836698\n",
      "3.161087358891422 0.9979144942648592 33.16413110593131\n",
      "3.156647576558045 0.9979144942648592 33.1167541668209\n",
      "3.1298646703781166 0.9979144942648592 33.12398556383529\n",
      "3.1308328400074212 0.9979144942648592 33.11937100696669\n",
      "3.1254853799796645 0.9979144942648592 33.120128317257176\n",
      "3.1182800468947325 0.9979144942648592 33.1056422588248\n",
      "3.1190728205448996 0.9979144942648592 33.09663658647221\n",
      "3.112006682505952 0.9979144942648592 33.0857966956803\n",
      "3.1102866026927134 0.9979144942648592 33.10972293626705\n",
      "3.1176219354462527 0.9979144942648592 33.09485958764034\n",
      "3.1214199358129053 0.9979144942648592 33.10238706023423\n",
      "3.123998924451411 0.9979144942648592 33.098756385287075\n",
      "3.1237468382390583 0.9979144942648592 33.10321115003361\n",
      "3.124067477878817 0.9979144942648592 33.102533094376334\n",
      "3.12217087538105 0.9979144942648592 33.10357323945786\n",
      "3.0580775037010572 0.9968717413972888 33.07119403690529\n",
      "3.070025097504523 0.9968717413972888 33.042118661355566\n",
      "3.055801530747035 0.9968717413972888 33.04929484983711\n",
      "3.0649446460891685 0.9968717413972888 33.02628967324006\n",
      "3.0536719984554064 0.9979144942648592 33.05226605419131\n",
      "3.0575681424625296 0.9968717413972888 33.02112125420014\n",
      "3.061251877507499 0.9979144942648592 33.049095930826006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrep_one_real_world\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 121\u001b[0m, in \u001b[0;36mrep_one_real_world\u001b[0;34m(idx, inflation_factor, cutoff)\u001b[0m\n\u001b[1;32m    113\u001b[0m avg_betas \u001b[38;5;241m=\u001b[39m expit(current_aug_state[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    115\u001b[0m complement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mavg_betas\n\u001b[0;32m--> 121\u001b[0m current_aug_state1, column_mod_11, column_mod_21, log_sigma_points_1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_targets_with_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_doc2vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_sigma_points_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m initial_targets \u001b[38;5;241m=\u001b[39m column_mod_11 \u001b[38;5;241m+\u001b[39m column_mod_21\n\u001b[1;32m    126\u001b[0m initial_targets \u001b[38;5;241m=\u001b[39m initial_targets\u001b[38;5;241m.\u001b[39mreshape(size_ens, train_lstm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mget_targets_with_weights\u001b[0;34m(batch_data, batch_data1, initial_ensembles, log_sigma_points_1)\u001b[0m\n\u001b[1;32m      3\u001b[0m n_hidden_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights_ann_1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m      5\u001b[0m hidden_weights_1 \u001b[38;5;241m=\u001b[39m initial_ensembles[:,:n_hidden_1]\u001b[38;5;241m.\u001b[39mreshape( size_ens, batch_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], h1)\n\u001b[0;32m----> 8\u001b[0m hidden_output_1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,kjl->kil\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_weights_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m hidden_layer_bias_1 \u001b[38;5;241m=\u001b[39m initial_ensembles[:,n_hidden_1:(n_hidden_1 \u001b[38;5;241m+\u001b[39m h1)]\u001b[38;5;241m.\u001b[39mreshape(size_ens, \u001b[38;5;241m1\u001b[39m,  h1)\n\u001b[1;32m     15\u001b[0m hidden_output_1 \u001b[38;5;241m=\u001b[39m hidden_output_1 \u001b[38;5;241m+\u001b[39m hidden_layer_bias_1\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/enkf/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[1;32m   1370\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rep_one_real_world(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
